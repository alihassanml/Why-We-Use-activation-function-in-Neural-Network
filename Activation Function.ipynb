{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d7bf9f-e4c7-415c-a58d-6f2c35217dc7",
   "metadata": {},
   "source": [
    "# **Why We Use activation function in Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170202fe-b8da-4245-b898-8ab3cf450d90",
   "metadata": {},
   "source": [
    "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it.\n",
    "\n",
    "1. **Non-linearity**: Activation functions introduce non-linearities into the model. Without them, the neural network would behave like a single-layer perceptron, regardless of the number of layers, as the composition of linear functions is still a linear function. Non-linearity allows the network to learn complex patterns.\n",
    "\n",
    "2. **Enabling Learning of Complex Patterns**: By introducing non-linear transformations, activation functions enable neural networks to approximate complex mappings between inputs and outputs, which is crucial for tasks such as image recognition, natural language processing, and more.\n",
    "\n",
    "3. **Controlling Output Range**: Many activation functions squash the output to a specific range (e.g., [0, 1] for sigmoid, [-1, 1] for tanh, and [0, ∞) for ReLU). This can help with the stability of the network and can make the training process more efficient.\n",
    "\n",
    "4. **Gradients for Backpropagation**: Activation functions also affect the gradients used in backpropagation. Functions like ReLU mitigate the vanishing gradient problem by providing a gradient of 1 for positive inputs, whereas functions like sigmoid can suffer from vanishing gradients as their derivatives are very small for large positive or negative inputs.\n",
    "\n",
    "5. **Sparsity**: Some activation functions, like ReLU, introduce sparsity in the network by outputting zero for negative inputs. This can make the network more efficient and can help with the interpretability of the model.\n",
    "\n",
    "Commonly used activation functions include:\n",
    "\n",
    "- **Sigmoid**: \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "- **Tanh**: \\( \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "- **ReLU (Rectified Linear Unit)**: \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "- **Leaky ReLU**: \\( \\text{Leaky ReLU}(x) = \\max(\\alpha x, x) \\) where \\( \\alpha \\) is a small constant\n",
    "- **Softmax**: Typically used in the output layer for classification tasks to convert logits into probabilities.\n",
    "\n",
    "Choosing the right activation function depends on the specific task and the architecture of the neural network."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b605b18-b7ac-4101-b1e1-09182e2d5ffd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b2506c4-f1dc-40d7-b3e5-1f933bfd17c1",
   "metadata": {},
   "source": [
    "1. **Sigmoid Activation Function**:\n",
    "   - **Formula**: \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "   - **Range**: (0, 1)\n",
    "   - **Properties**: The sigmoid function maps any input value to a value between 0 and 1. It's often used in the output layer of binary classification problems.\n",
    "   - **Graph**: The sigmoid curve is S-shaped and asymptotically approaches 0 and 1.\n",
    "\n",
    "2. **Tanh (Hyperbolic Tangent) Activation Function**:\n",
    "   - **Formula**: \\( \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "   - **Range**: (-1, 1)\n",
    "   - **Properties**: Tanh is similar to the sigmoid function but maps input values to a range between -1 and 1. It is zero-centered, which helps in having a more balanced gradient.\n",
    "   - **Graph**: The tanh curve is also S-shaped but symmetric around the origin.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit) Activation Function**:\n",
    "   - **Formula**: \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "   - **Range**: [0, ∞)\n",
    "   - **Properties**: ReLU outputs the input directly if it is positive; otherwise, it outputs zero. This function is widely used because it helps to mitigate the vanishing gradient problem and introduces sparsity in the network.\n",
    "   - **Graph**: The ReLU function is linear for positive values and flat for negative values.\n",
    "\n",
    "4. **Leaky ReLU Activation Function**:\n",
    "   - **Formula**: \\( \\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "      x & \\text{if } x \\ge 0 \\\\\n",
    "      \\alpha x & \\text{if } x < 0 \n",
    "      \\end{cases} \\)\n",
    "   - **Range**: (-∞, ∞)\n",
    "   - **Properties**: Similar to ReLU, but it allows a small gradient when the input is negative. This small slope (controlled by \\(\\alpha\\)) helps to keep the gradient flow alive, preventing the dying ReLU problem.\n",
    "   - **Graph**: The Leaky ReLU function is linear for positive values and has a small slope for negative values.\n",
    "\n",
    "5. **Softmax Activation Function**:\n",
    "   - **Formula**: \\( \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\)\n",
    "   - **Range**: (0, 1) for each output, and the sum of all outputs is 1\n",
    "   - **Properties**: The softmax function is used in the output layer of a network for multi-class classification problems. It converts logits into probabilities, with the output vector summing to 1.\n",
    "   - **Graph**: The softmax function doesn't have a single graph because it operates on a vector. Each element of the output is a fraction of the exponentiated input relative to the sum of all exponentiated inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
